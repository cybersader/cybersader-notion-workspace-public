# Improving query expansion using pseudo-relevant web knowledge for information retrieval

## Abstract

In the field of information retrieval, [query expansion](https://www.sciencedirect.com/topics/computer-science/query-expansion) (QE) has long been used as a technique to deal with the fundamental issue of word mismatch between a user’s query and the target information. In the context of the relationship between the query and expanded terms, existing weighting techniques often fail to appropriately capture the term-term relationship and term to the whole query relationship, resulting in low retrieval effectiveness. Our proposed QE approach addresses this by proposing three weighting models based on (1) tf-idf, (2) k-nearest neighbor (kNN) based [cosine similarity](https://www.sciencedirect.com/topics/computer-science/cosine-similarity), and (3) correlation score. Further, to extract the initial set of expanded terms, we use pseudo-relevant web knowledge consisting of the top � web pages returned by the three popular search engines namely, Google, Bing, and DuckDuckGo, in response to the original query. Among the three weighting models, *tf-idf* scores each of the individual terms obtained from the web content, *kNN-based [cosine similarity](https://www.sciencedirect.com/topics/computer-science/cosine-similarity)* scores the expansion terms to obtain the term-term relationship, and *correlation score* weighs the selected expansion terms with respect to the whole query. The proposed model, called web knowledge based query expansion (WKQE), achieves an improvement of 25.89% on the Mean Average Precision (MAP) score and 30.83% on the Geometric Mean Average precision (GMAP) score over the unexpanded queries on the FIRE dataset. A comparative analysis of the WKQE techniques with other related approaches clearly shows significant improvement in the retrieval performance. We have also analyzed the effect of varying the number of pseudo-relevant documents and expansion terms on the retrieval effectiveness of the proposed model.

## Proposed Approach

During query expansion, the first important decision is the determination of the source for mining candidate expansion terms. The top-ranked documents retrieved in response to the initial query appeal be a good source for mining candidate expansion terms. In the context of pseudo relevance feedback, these documents form the set of pseudo-relevant documents. Our proposed approach, called web knowledge based query expansion (wkqe) and shown in fig. 1, is a pseudorelevant feedback based technique, where the pseudo-relevant documents consists of the web-pages of the top n urls returned by three popular search engines namely: google, bing, and duckduckgo, in response to the initial query. The motivation for doing so has already been discussed earlier in sec. 1. The relevant terms found in the collection of these pseudo-relevant documents are used for qe. Sometimes a particular search engine may not provide the result that the user exactly intended. For example, consider the top ten search results on the query term apple as returned by the three search engines. While google and bing provide results interpreting apple only as a company, duckduckgo offers results interpreting the query term both as a company as well as the fruit. So, for diversifying the sense of expansion terms we select three popular search engines instead of just one. Both term-to-term and term to the whole query relationships are computed for finding the most relevant in the set of candidate expansion terms. To estimate the term-to-term relationship, we weigh the expansion terms with the proposed tf-itf and knn based cosine similarity score. To estimate the term to the whole query relationship, we weigh the expansion terms with the correlation score. As shown in fig. 1, the proposed approach consist of five main steps: (i) retrieval of top n urls, (ii) text collection and tokenization, (iii) weighting with tf-itf, (iv) weighting with knn-based approach, and (v) reweighting with correlation score. These steps are described next. 3.1. Retrieval of top n urls in order to expand the initial query, first of all we fired the initial query on three popular search engines namely: google, bing, and duckduckgo. After that, we extracted the web pages corresponding to the top n urls returned by each of the search engine separately. These web pages act as the set of pseudo-relevant documents for our approach. When considering the top n urls, we have excluded the urls associated with advertising, video, and e-commerce sites. For experimental evaluation, we considered different value of n as 5, 10, 15, 20, 30, 50. However, the proposed model showed the best performance for n = 20. See sec. 5.1 for more details. 3.2. Text collection and tokenization the entire content of the pseudo-relevant web pages, corresponding to the top n urls, is not informative. A web page usually has different types of content that are either not related to the topic of the web page or are not useful for the purpose of query expansion. Such items can be: decoration: pictures, animations, logos, etc. For attractions or advertising purposes. Navigation: intra and inter hyperlinks to guide the user in different parts of a web page. Interaction: forms to collect user information or provide search services. - other special words or paragraphs such as copyrights and contact information. In the context of query matching and term weighting, all of the points as mentioned above are considered to be noise and can adversely affect the retrieval performance. For example, if some words from an advertisement embedded in a top-ranked pseudo-relevant web page are chosen as expansion terms, irrelevant documents containing these advertising terms can be ranked highly. Therefore, it is necessary to filter out the noisy information in web pages and retrieve only the contents that are semantically related to the initial query. The document object model (dom)12 represents the structure of an html page as a tag tree. Dom-based segmentation approaches have been widely used for many years. In our proposed wkqe approach, we use these page segmentation methods where the tags that can represent useful blocks in a page include p (for paragraph), h1h6 (for heading), t able (for table), ul (for list), etc. To extract the relevant text contents from the web pages (html and xml documents), we used beautiful soup13 python library and web services. After collecting the text content of the web pages returned by the three search engines, we created a combined corpus c of all the web pages. Then, we tokenized corpus c to identify individual words. For tokenisation and to remove stop words, we used the natural language toolkit (nltk)14. The part of speech (pos) tags assigned by nltk tagger were used to identify phrases and individual words. After the extraction of individual terms, we weighted these individual terms with tf-itf. The weighting of the expansion terms is described next. 3.3. Weighting with tf-itf we weighted the individual terms with tf-itf, which is a modified version of tf-idf. This weight is a statistical measure used to evaluate how important a word in a corpus is. The term frequency (tf) measures how frequently a term occurs in a corpus. While computing tf, all terms are considered equally important. However, it is known that certain terms in a corpus, such as is, of, and that, may appear a lot of times but have little importance. Thus we need to weight down such frequent terms while scale up the importance of the rare ones. This is achieved by computing the inverse term frequency (itf). We weight the individual terms with tf-itf scoring function as follows: score(ti) = tf(ti , c) . Itf(ti , c) = tf(ti , c) . Log t |ti | (1) where: tf(ti , c) denotes the term frequency of term ti in the entire corpus c, itf(ti , c) denotes the inverse term frequency of ti in the entire corpus c, t is the number of terms in the entire corpus c, and |ti | is the number of times term ti appears in the entire corpus c. After assigning this score to each of the terms in the corpus, we ranked these terms according to the scoring value. Then, we selected the top m individual terms as intermediate candidate expansion terms. After this, these intermediate candidate expansion terms were re-weighted with the knn-based cosine similarity score, which is described next. 3.4. Weighting with knn-based approach the knn-based approach weights the intermediate candidate expansion terms with cosine similarity and selects the top k nearest neighbor candidate expansion terms. It establishes a term-term correlation among the candidate expansion terms so that the most relevant expansion terms can be chosen. The proposed knn-based approach is an extension of the technique given by roy et algorithm 1 k-nearest neighbors input: cexp set of intermediate candidate expansion terms sorted using eq.1. K number of nearest neighbor candidate expansion terms to be returned. L number of terms terms to be dropped during each iteration. Output: nn set of iteratively added nearest neighbor candidate expansion terms. 1: initialisation: nn {}; r 5 # r number of iterations. 2: select t cexp having maximum score 3: while r > 0 do 4: nn nn {t} # add t to nn. 5: cexp cexp {t} # remove the term t from cexp. 6: sort cexp w. R. T. T using cosine similarity score of eqn. (2) 7: select t cexp having maximum score 8: select cl cexp as the set of l least scoring terms in cexp 9: cexp cexp cl # remove the set of l least neighbors terms from cexp. 10: select t cexp having maximum score 11: r r 1 12: end while 13: select ckr cexp as the set of k highest scoring terms in cexp 14: nn nn ckr 15: return nn # final set of nearest neighbor candidate expansion terms. Al. [44]. Here, instead of computing the nearest neighbors for each query term, we computed the nearest neighbors of each intermediate candidate expansion term extracted in response to the original query. The highly similar neighbors have comparatively lower drift in terms of similarity than the terms occurring later in the list. Since the most similar terms are the strongest contenders for becoming the expansion terms, it can be assumed that these terms are also similar to each other, in addition to being similar to the query term. Based on this, we use an iterative process for sorting the expansion terms described in algorithm 1. Algorithm 1 takes as input the sorted (using eq. 1) set of candidate expansion terms obtained from the previous step, denoted as cexp. First, the expansion term having maximum similarity score in cexp is identified as t (line 2 in algorithm 1). Term t acts as the nearest neighbor for the first iteration of the iterative process (lines 6-9). At the start of each iteration, the nearest neighbor t is added to the set of nearest neighbors nn, which is initialized as empty (line 1), and removed from the set of intermediate candidate expansion terms cexp (lines 4 and 5). The terms in cexp are then sorted based on their proximity with t computed on the basis of cosine similarity. The cosine similarity between two terms tk and ti is given as: simcosine(tk, ti) = q ctk, ti p dj w2 tk, j . P dj w2 ti, j p dj wtk, j . Wti, j qp dj w2 tk, j . P dj w2 ti, j (2) where: ctk, ti denotes the correlation score of term tk with ti in the entire corpus c and wti, j (wtk, j ) is the weight of term ti (tk) in the document dj (as returned by one of the three search engines). Wti, j is computed as (wtk, j is similarly defined): wti, j = tf(ti , j) . Itf(ti , j) = tf(ti , j) . Log t |dtj | (3) where: tf(ti , j) denotes the term frequency of ti in the document dj . Itf(ti , j) is the inverse term frequency of ti in the document dj . |dtj | is the number of distinct terms in the document dj , and t is the number of terms in the entire collection. Then, the l least similar neighbors are removed from the intermediate set of candidate expansion terms cexp. This completes the first iteration. This iterative process is executed for r iterations. In each iteration, the nearest neighbors list is rearranged on the basis of the nearest neighbor obtained from the previous iteration and a set of l least similar neighbors is eliminated. Essentially, by following the above procedure, we are compelling the nearest neighbors to be similar to each other in addition to being similar to the original query. A high value of r 10 may lead to query drift, while a low value r 2 essentially performs similar to the initial set of intermediate candidate expansion terms. In our proposed method we chose r = 5 as the number of iterations. Finally, at the end of r iterations, the top k nearest neighbor candidate expansion terms are returned as the final set of nearest neighbor candidate expansion terms (lines 13 and 14). Now these intermediate candidate expansion terms are reweighted using correlation score (described next) and the top n terms are chosen as the final set of expansion terms. 3.5. Reweighting with correlation score so far a set of candidate expansion terms have been obtained, where each expansion term is strongly connected to the other individual candidate expansion terms. These terms have been allocated weights using tf-itf and the knn-based approach. Things done so far resolve the issue of semantic similarity between term-to-term relationship. However, this may not accurately reflect the relationship of an expansion term to the query as a whole. For example, while the word program may be highly associated with the word computer, use of this association for selecting candidate expansion terms may work well for some queries such as java program and application program but not for others such as space program, tv program, and government program. This problem has been analyzed in bai et al. [45]. To address this language ambiguity problem, we use a weighting scheme called correlation score. A similar approach has been suggested in xu and croft [46], cui et al. [26], sun et al. [47], and, azad and deepak [22]. This approach extends the term-to-term association methods described previously in sections 3.3 and 3.4. In this approach we used term-to-term correlation and computed the correlation score of a given candidate expansion term ti to each query term. We then combined the found score to find the correlation to the initial query q. The correlation score is defined as follows. Let q be the original query having individual terms qks and let ti be a candidate expansion term. Then, the correlation score of ti with q, denoted cti, q, is computed as: cti, q = 1 |q| . X qkq cti, qk 1 |q| . X qkq x dj wti, j . Wqk, j (4) where: cti, qk is the correlation (similarity) score between the candidate expansion term ti and the query term qk and wti, j (wqk, j ) is the weight of term ti (qk) in the document dj . The weight of the candidate expansion term ti in the document dj , denoted wti, j (wqk, j is similarly defined), is computed as: wti, j = tf(ti , j) . Itf(ti , j) = tf(ti , j) . Log t |dtj | (5) where: tf(ti , j) denotes the term frequency of ti in the document dj . Itf(ti , j) is the inverse term frequency of ti in the document dj . |dtj | is the number of distinct terms in the document dj , and t is the number of terms in the entire collection. After assigning the correlation score to candidate expansion terms, we collect the top n terms as the final set of candidate expansion terms.