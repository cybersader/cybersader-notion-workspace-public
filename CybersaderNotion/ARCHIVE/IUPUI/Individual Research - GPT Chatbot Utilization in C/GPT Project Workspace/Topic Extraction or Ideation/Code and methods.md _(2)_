# Code and methods

## Generating Text from Text GPT 2

Based on the paper "Query expansion with artificially generated texts", the method involves using a GPT-2 model to generate texts from a seed query. These generated texts, which are different even if the seed query is the same, are then concatenated and used as the expanded query for an Information Retrieval (IR) system like BM25.

The paper suggests that this method provides not only new terms for the query, but also a better estimate of their relative weights, the importance of the original query words, and that it can be used in specialized domains.

Here's a basic code outline to get you started:

```python
# Required Libraries
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load GPT-2 model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Given Query
query = "your_query"

# Generate 100 texts per query
generated_texts = []
for i in range(100):
    inputs = tokenizer.encode(query, return_tensors='pt')
    outputs = model.generate(inputs, max_length=512, temperature=0.5, top_p=0.95, top_k=40, do_sample=True)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    generated_texts.append(generated_text)

# Concatenate the generated texts
expanded_query = ' '.join(generated_texts)

# Now you can use this expanded_query with any Information Retrieval system like BM25
```

This example assumes that you have the transformers library installed and that you have access to the GPT-2 model.

The parameters for the text generation (length=512, temperature=0.5, top p=0.95, top k = 40) were taken from the paper and are set to influence the creativity of the text generation.

Please note that this is a simplified example and doesn't cover all the nuances of the method described in the paper. For instance, the paper mentions the use of a fine-tuned GPT-2 model for domain-specific collections, which isn't included in this example.

Also, note that the generation of a large number of texts for each query might be time-consuming, and you might want to consider using a smaller number of generated texts or a more efficient method depending on your specific use case.

The paper also suggests that there are many possible improvements and modifications to this basic method, such as optimizing the GPT model parameters or exploring fine-tuning capabilities more thoroughly. Therefore, you might want to experiment with different settings and approaches to find what works best for your specific task.

# Using GPT 2 Text Appended to Query with BM25 to extract topics (requires initial prompt)

Here's a simple way to use the **`rank_bm25`** library in conjunction with a GPT-2 model to perform query expansion and topic extraction.

In the example below, we will:

1. Tokenize a corpus of documents.
2. Use the GPT-2 model to generate additional text based on a given query.
3. Add the generated text to the query.
4. Use the expanded query to retrieve the most relevant documents from the corpus using BM25.

```python
from rank_bm25 import BM25Okapi
from transformers import pipeline

# Predefined corpus
corpus = [
    "The sky is blue and beautiful.",
    "Love this blue and beautiful sky!",
    "The quick brown fox jumps over the lazy dog.",
    "A king's breakfast has sausages, ham, bacon, eggs, toast and beans",
    "I love green eggs, ham, sausages and bacon!",
    "The brown fox is quick and the blue dog is lazy!",
    "The sky is very blue and the sky is very beautiful today",
    "The dog is lazy but the brown fox is quick!"
]

# Tokenize corpus
tokenized_corpus = [doc.split(" ") for doc in corpus]

# Initialize BM25
bm25 = BM25Okapi(tokenized_corpus)

# Initial query
query = "blue sky"

# Set up GPT-2 text generation model
text_generator = pipeline('text-generation', model='gpt-2')

# Generate additional text based on the query
generated_texts = text_generator(query, max_length=100, do_sample=True, temperature=0.7)

# Add generated text to query
expanded_query = query + " " + generated_texts[0]['generated_text']

# Tokenize expanded query
tokenized_query = expanded_query.split(" ")

# Get BM25 scores for each document in the corpus
doc_scores = bm25.get_scores(tokenized_query)

# Print scores
for i, score in enumerate(doc_scores):
    print(f"Score for document {i + 1}: {score}")
```

This script uses the **`pipeline`** feature from the **`transformers`** library to set up a GPT-2 text generation model. It then generates additional text based on the initial query and adds this text to the query to expand it. The expanded query is then used to score each document in the corpus using BM25.

Note that this is a basic example and may not work perfectly for all queries and corpora. You may need to further refine the text generation and scoring processes to get better results. For example, you might want to use a different GPT-2 model, fine-tune the model on your specific corpus, or adjust the parameters of the BM25 algorithm.

If you don't have an initial query and you want to extract topics from your corpus of documents, you may want to look into topic modeling techniques like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF). These techniques can identify a set of topics that are present in your corpus and assign each document to one or more topics.

Here's an example of how you might use LDA with the Gensim library to extract topics from your corpus:

```python
import gensim
from gensim import corpora
from gensim.models import LdaModel
from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation

# Preprocess documents
docs = [strip_punctuation(remove_stopwords(doc.lower())) for doc in corpus]

# Split documents into tokens
texts = [doc.split() for doc in docs]

# Create a dictionary from the tokens
dictionary = corpora.Dictionary(texts)

# Create a bag of words corpus
corpus_bow = [dictionary.doc2bow(text) for text in texts]

# Train an LDA model
lda_model = LdaModel(corpus_bow, num_topics=10, id2word=dictionary)

# Print the topics
for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))
```

In this script, each document in the corpus is preprocessed by converting to lowercase, removing punctuation and stop words, and splitting into tokens. These tokens are used to create a dictionary and a bag of words corpus, which are then used to train an LDA model. The model is set to identify 10 topics (but you can adjust this number as needed).

The script then prints out the 10 topics that the model identified. Each topic is represented as a list of the most important words in that topic, along with their relative importance.

Note that this is a very basic example, and you may need to refine the preprocessing and model training processes to get better results. For example, you might want to use more advanced tokenization, remove more types of noise from your documents, or adjust the parameters of the LDA model.

## No initial prompt using LDA

If you don't have an initial query and you want to extract topics from your corpus of documents, you may want to look into topic modeling techniques like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF). These techniques can identify a set of topics that are present in your corpus and assign each document to one or more topics.

Here's an example of how you might use LDA with the Gensim library to extract topics from your corpus:

```python
import gensim
from gensim import corpora
from gensim.models import LdaModel
from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation

# Preprocess documents
docs = [strip_punctuation(remove_stopwords(doc.lower())) for doc in corpus]

# Split documents into tokens
texts = [doc.split() for doc in docs]

# Create a dictionary from the tokens
dictionary = corpora.Dictionary(texts)

# Create a bag of words corpus
corpus_bow = [dictionary.doc2bow(text) for text in texts]

# Train an LDA model
lda_model = LdaModel(corpus_bow, num_topics=10, id2word=dictionary)

# Print the topics
for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))
```

In this script, each document in the corpus is preprocessed by converting to lowercase, removing punctuation and stop words, and splitting into tokens. These tokens are used to create a dictionary and a bag of words corpus, which are then used to train an LDA model. The model is set to identify 10 topics (but you can adjust this number as needed).

The script then prints out the 10 topics that the model identified. Each topic is represented as a list of the most important words in that topic, along with their relative importance.

Note that this is a very basic example, and you may need to refine the preprocessing and model training processes to get better results. For example, you might want to use more advanced tokenization, remove more types of noise from your documents, or adjust the parameters of the LDA model.

Yes, the Latent Dirichlet Allocation (LDA) algorithm used in the script provided indeed generates topics. It does so by treating each document as a mixture of topics, and each topic as a mixture of words. This allows it to identify which words are most likely to appear together in the same document, and thus constitute a topic.

Each "topic" in LDA is represented as a distribution over words. The words with the highest probabilities for a given topic are considered the most representative words of that topic. For example, if the model generates a topic where the words "rain", "sun", "weather", and "temperature" have the highest probabilities, we could interpret this topic as being about weather.

The output from the **`print_topics`** method in the script is a list of the top words for each topic, along with the probabilities of those words in that topic. This gives you a sense of what each topic is about. However, interpreting the topics requires some human judgement, as the LDA model itself doesn't assign any labels or meanings to the topics it identifies.

Also, keep in mind that topic modeling is a form of unsupervised machine learning, and it may not always produce perfect or clear results. It's often necessary to experiment with different approaches or parameters to get the best results for your specific data and use case.