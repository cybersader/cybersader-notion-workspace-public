# Code and methods

# Mentioned Methods

Based on the research paper, there are several methods for query expansion:

1. **Ontology**: It's a method where related terms and their relationships are predefined. You could use ontology-based libraries such as **`pyLODE`** or **`owlready2`** in Python. You would need an ontology relevant to your data.
2. **Association Rules**: This method often uses market basket analysis to determine how concepts are commonly paired in your data. The **`mlxtend`** library can be used for association rule mining in Python.
3. **Wordnet/Synonym mapping**: Wordnet is a lexical database of English words, which can be used to find the meanings of words, synonyms, antonyms, etc. You can use the NLTK's WordNet interface. For synonym mapping, you might need a comprehensive dictionary of synonyms relevant to your data.
4. **Metathesaurus**: This method requires a metathesaurus like UMLS (Unified Medical Language System). This could be useful if your data is medical in nature.
5. **Concept-based**: This approach involves representing documents in your data as a set of concepts instead of words. This can be done with a tool like MetaMap, which maps English text to concepts in the UMLS Metathesaurus.
6. **Local Co-occurrence**: This method involves finding terms that frequently appear close together in your data. This can be done using techniques like n-grams or collocations in NLTK.
7. **Latent Semantic Indexing (LSI)**: LSI uses Singular Value Decomposition (SVD) on the document-term matrix to identify patterns and relationships between terms and concepts in your data. You can use libraries like **`gensim`** to perform LSI in Python.

**Note**: All these methods have their own limitations and might require substantial preprocessing and tuning to work well on your specific data. For instance, ontologies and metathesauri need to be specific to the domain of your data. Association rules and local co-occurrence methods require significant computational resources for large datasets. Synonym mapping and WordNet can lead to over-expansion, where too many related terms are added to the query, diluting its original focus.

Once you have expanded the queries using the above methods, you can use the search engine techniques like the BM25 algorithm we discussed earlier to rank the documents based on relevance to the expanded query.

As for extracting topics from the inputted markup/HTML/data from a website, you can use the topic modelling techniques we discussed earlier, such as Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), implemented in the **`gensim`** and **`sklearn`** libraries respectively. You would first need to preprocess the data (e.g., removing HTML tags, tokenizing, removing stopwords, etc.) before applying these techniques.

# Topic Extraction and Ranking

Techniques such as Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF) don't inherently provide a ranking of topics. Instead, they assign a probability distribution over topics for each document and a probability distribution over words for each topic.

If you're looking to rank topics, one common approach is to rank them based on their prevalence. A topic is considered more prevalent if it has a higher probability for a larger number of documents. You can calculate the average topic probability across all documents for each topic and then rank the topics based on this measure.

Here's a simplified example using LDA in **`gensim`** library:

```python
from gensim import corpora, models
import operator

# Assume texts is your preprocessed data
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Train LDA model
lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary)

# Get the topic distribution for each document
doc_topics = lda_model.get_document_topics(corpus)

# Initialize a list to hold the total probability for each topic
topic_probabilities = [0] * lda_model.num_topics

# Sum up the probabilities for each topic
for doc in doc_topics:
    for topic, prob in doc:
        topic_probabilities[topic] += prob

# Calculate the average probability for each topic
topic_probabilities = [prob / len(corpus) for prob in topic_probabilities]

# Rank the topics
ranked_topics = sorted(enumerate(topic_probabilities), key=operator.itemgetter(1), reverse=True)

for i, prob in ranked_topics:
    print(f'Topic {i} - Average Probability: {prob}')
    print(lda_model.print_topic(i))  # Print the top words associated with the topic
```

This will give you a ranked list of topics based on their average probabilities across all documents, as well as the top words associated with each topic.

Please remember, in real applications, text data usually requires substantial preprocessing before feeding it to an LDA model. This includes steps like tokenization, removing stop words, stemming/lemmatization, etc.